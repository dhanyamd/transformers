{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cea9e249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import math \n",
    "import torch.nn.functional as F \n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04f14061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_masked_attention(\n",
    "        values: torch.Tensor,\n",
    "        keys: torch.Tensor,\n",
    "        query: torch.Tensor,\n",
    "        mask: torch.Tensor = None \n",
    "): \n",
    "    attention_scores = torch.matmul(query, keys.transpose(-2,-1)) \n",
    "    attention_scores = attention_scores / math.sqrt(keys.shape[-1])\n",
    "    if mask is not None: \n",
    "        attention_scores = torch.where(mask == 0, torch.tensor(-1e9), attention_scores) \n",
    "    attention_scores = F.softmax(attention_scores, dim=-1) \n",
    "    attention = torch.matmul(attention_scores, values) \n",
    "    return attention, attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62574633",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module): \n",
    "    def __init__(self, embed_size: int):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(embed_size, embed_size)\n",
    "        self.layer2 = nn.Linear(embed_size, embed_size) \n",
    "    def forward(self, x): \n",
    "        x = self.layer1(x) \n",
    "        x = F.gelu(x)\n",
    "        x = self.layer2(x) \n",
    "        return x \n",
    "class AttentionLayer(nn.Module): \n",
    "    def __init__(self,embed_size: int):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size \n",
    "        self.query_dense = nn.Linear(embed_size, embed_size) \n",
    "        self.key_dense = nn.Linear(embed_size, embed_size) \n",
    "        self.value_dense = nn.Linear(embed_size, embed_size)\n",
    "        self.output_dense = nn.Linear(embed_size, embed_size) \n",
    "    def forward(self, embeddings: torch.Tensor): \n",
    "        batch_size = embeddings.shape[0] \n",
    "        seq_length = embeddings.shape[1] \n",
    "        query = self.query_dense(embeddings)\n",
    "        key = self.key_dense(embeddings)\n",
    "        value = self.value_dense(embeddings)\n",
    "        right_triangular_mask = torch.tril(torch.ones((1, seq_length, seq_length))).to(embeddings.device)\n",
    "        attention, attention_scores = calculate_masked_attention(value, key, query, right_triangular_mask) \n",
    "        return attention, attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c58a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module): \n",
    "    def __init__(self, embed_size: int):\n",
    "        super().__init__()\n",
    "        self.attention_layer = AttentionLayer(embed_size) \n",
    "        self.feed_forward = FeedForward(embed_size) \n",
    "        self.layer_norm1 = nn.LayerNorm(embed_size) \n",
    "    def forward(self, x: torch.Tensor): \n",
    "        context, attention_scores = self.attention_layer(x)\n",
    "        context = self.layer_norm1(context) \n",
    "        context = self.feed_forward(context) \n",
    "        context = F.gelu(context) \n",
    "        output = context + x \n",
    "        return output, attention_scores \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_size: int, num_layers: int): \n",
    "        super().__init__() \n",
    "        self.transformers_blocks = nn.ModuleList([TransformerBlock(embed_size) for _ in range(num_layers)]) \n",
    "    def forward(self, x: torch.Tensor): \n",
    "        attention_scores = [] \n",
    "        for transformer_block in self.transformers_blocks: \n",
    "            x, attention_score = transformer_block(x) \n",
    "            attention_scores.append(attention_score) \n",
    "        return x, attention_scores\n",
    "    \n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size: int, max_seq_length: int):\n",
    "        super().__init__() \n",
    "        position = torch.arange(max_seq_length).unsqueeze(1) \n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2) * (-math.log(1000.0) / embed_size))\n",
    "        pe = torch.zeros(20, embed_size) \n",
    "        pe[:, 0::2] = torch.sin(position * div_term) \n",
    "        pe[:, 1:: 2] = torch.cos(position * div_term) \n",
    "        self.register_buffer('position_embedding', pe)\n",
    "    def forward(self, x: torch.Tensor): \n",
    "        return x + self.position_embedding[:x.size(1), :] \n",
    "\n",
    "class CasualLanguageModel(nn.Module): \n",
    "    def __init__(self, embed_size: int, vocab_size: int, num_layers: int): \n",
    "        super().__init__() \n",
    "        self.embedding_layer = nn.Parameter(torch.randn(vocab_size, embed_size))\n",
    "        self.transformer = Transformer(embed_size, num_layers)\n",
    "        self.positional_encoding = SinusoidalPositionalEncoding(embed_size, max_seq_length=20) \n",
    "\n",
    "    def forward(self, x: torch.Tensor, return_attention_scores: bool = False): \n",
    "        x = torch.nn.functional.embedding(x, self.embedding_layer)\n",
    "        x = self.positional_encoding(x) \n",
    "        x, attention_scores = self.transformer(x) \n",
    "        logits = torch.matmul(x, self.embedding_layer.T)\n",
    "        if return_attention_scores: \n",
    "            return logits, attention_scores\n",
    "        return logits \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cce764cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<start>', '<end>', 'dont', 'fun', 'if', 'you', 'is', 'learning', 'machine', 'forget', 'subscibe', 'like', 'awesome', 'i', 'to', 'than', 'more', 'and']\n",
      "vocab size:  {'<pad>': 0, '<start>': 1, '<end>': 2, 'dont': 3, 'fun': 4, 'if': 5, 'you': 6, 'is': 7, 'learning': 8, 'machine': 9, 'forget': 10, 'subscibe': 11, 'like': 12, 'awesome': 13, 'i': 14, 'to': 15, 'than': 16, 'more': 17, 'and': 18}\n"
     ]
    }
   ],
   "source": [
    "dataset = [\n",
    "    \"dont forget to like and subscibe\",\n",
    "    \"dont forget machine learning is fun\",\n",
    "    \"machine learning is fun and awesome\",\n",
    "    \"if you like machine learning i like you\",\n",
    "    \"i like you more than machine learning\"\n",
    "]\n",
    "vocab = set() \n",
    "special_tokens = [\"<pad>\", \"<start>\", \"<end>\"] \n",
    "for sentence in dataset: \n",
    "    vocab.update(sentence.split())\n",
    "vocab = special_tokens + list(vocab) \n",
    "vocab_to_index = {word: index for index, word in enumerate(vocab)} \n",
    "vocab_size = len(vocab) \n",
    "print(vocab)\n",
    "print(\"vocab size: \", vocab_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e06d058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  3, 10, 15, 12, 18, 11,  2,  0,  0],\n",
       "        [ 1,  3, 10,  9,  8,  7,  4,  2,  0,  0],\n",
       "        [ 1,  9,  8,  7,  4, 18, 13,  2,  0,  0],\n",
       "        [ 1,  5,  6, 12,  9,  8, 14, 12,  6,  2],\n",
       "        [ 1, 14, 12,  6, 17, 16,  9,  8,  2,  0]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(sentence: str): \n",
    "    return [vocab_to_index[word] for word in sentence.split()]\n",
    "def encode_batch(sentences: list[str]): \n",
    "    encoded_sentences = [[vocab_to_index[\"<start>\"]] + encode(sentence) + [vocab_to_index[\"<end>\"]] for sentence in sentences] \n",
    "    max_length = max([len(encoded_sentence) for encoded_sentence in encoded_sentences])\n",
    "    encoded_sentences = [encoded_sentence + [vocab_to_index[\"<pad>\"]] * (max_length - len(encoded_sentence)) for encoded_sentence in encoded_sentences ]\n",
    "    return encoded_sentences \n",
    "def decode(tokens: list[int]): \n",
    "    return \" \".join([vocab[token] for token in tokens]) \n",
    "tokenized_dataset = encode_batch(dataset) \n",
    "tokenized_dataset = torch.tensor(tokenized_dataset) \n",
    "(tokenized_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8646fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> dont forget to like and subscibe <end> <pad> <pad>'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d01188e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  3, 10, 15, 12, 18, 11,  2,  0],\n",
      "        [ 1,  3, 10,  9,  8,  7,  4,  2,  0],\n",
      "        [ 1,  9,  8,  7,  4, 18, 13,  2,  0],\n",
      "        [ 1,  5,  6, 12,  9,  8, 14, 12,  6],\n",
      "        [ 1, 14, 12,  6, 17, 16,  9,  8,  2]])\n",
      "tensor([[ 3, 10, 15, 12, 18, 11,  2,  0,  0],\n",
      "        [ 3, 10,  9,  8,  7,  4,  2,  0,  0],\n",
      "        [ 9,  8,  7,  4, 18, 13,  2,  0,  0],\n",
      "        [ 5,  6, 12,  9,  8, 14, 12,  6,  2],\n",
      "        [14, 12,  6, 17, 16,  9,  8,  2,  0]])\n"
     ]
    }
   ],
   "source": [
    "input_tokens = tokenized_dataset[:, :-1] \n",
    "target_tokens = tokenized_dataset[:, 1:] \n",
    "print(input_tokens) \n",
    "print(target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd170d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  <start> dont forget to like and subscibe <end> <pad>\n",
      "Target:  dont forget to like and subscibe <end> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \", decode(input_tokens[0].tolist())) \n",
    "print(\"Target: \", decode(target_tokens[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43b2850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab) \n",
    "embed_size = 6 \n",
    "num_layers = 2\n",
    "device = \"cpu\"\n",
    "num_epochs = 600 \n",
    "input_tokens = input_tokens.to(device) \n",
    "target_tokens = target_tokens.to(device) \n",
    "casual_language_model = CasualLanguageModel(embed_size=embed_size, vocab_size=vocab_size, num_layers=num_layers).to(device)\n",
    "optimizer = torch.optim.Adam(casual_language_model.parameters(), lr=2e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9ad2222",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = casual_language_model(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "164594b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> 0.15068207681179047\n",
      "<start> 6.758410017937422e-05\n",
      "<end> 0.002768024103716016\n",
      "dont 0.01829790137708187\n",
      "fun 0.014354881830513477\n",
      "if 0.17056222259998322\n",
      "you 8.814829925540835e-05\n",
      "is 0.0007069381535984576\n",
      "learning 0.00017167140322271734\n",
      "machine 0.0028994965832680464\n",
      "forget 0.31739717721939087\n",
      "subscibe 0.00017451250459998846\n",
      "like 0.0036833747290074825\n",
      "awesome 0.2790459394454956\n",
      "i 0.008447432890534401\n",
      "to 0.00040081856423057616\n",
      "than 0.013626216910779476\n",
      "more 0.0047680982388556\n",
      "and 0.011857414618134499\n"
     ]
    }
   ],
   "source": [
    "for v, x in zip(vocab, logits[0][2].softmax(-1)): \n",
    "    print(v, x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c47bfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 9, 19])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([45, 19])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(logits.shape) \n",
    "logits.view(-1, logits.shape[-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6bcf7887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 9])\n",
      "torch.Size([45])\n"
     ]
    }
   ],
   "source": [
    "print(target_tokens.shape)\n",
    "print(target_tokens.reshape(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ce19b5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3, 10, 15, 12, 18, 11,  2,  0,  0,  3, 10,  9,  8,  7,  4,  2,  0,  0,\n",
       "         9,  8,  7,  4, 18, 13,  2,  0,  0,  5,  6, 12,  9,  8, 14, 12,  6,  2,\n",
       "        14, 12,  6, 17, 16,  9,  8,  2,  0])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tokens.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "59770dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.8332, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), target_tokens.reshape(-1))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4b02fb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 6.833237171173096\n",
      "Epoch 10, Loss: 6.458953857421875\n",
      "Epoch 20, Loss: 6.134280681610107\n",
      "Epoch 30, Loss: 5.807788848876953\n",
      "Epoch 40, Loss: 5.470464706420898\n",
      "Epoch 50, Loss: 5.160411357879639\n",
      "Epoch 60, Loss: 4.869407653808594\n",
      "Epoch 70, Loss: 4.580451965332031\n",
      "Epoch 80, Loss: 4.282816410064697\n",
      "Epoch 90, Loss: 3.991530418395996\n",
      "Epoch 100, Loss: 3.7186803817749023\n",
      "Epoch 110, Loss: 3.461916208267212\n",
      "Epoch 120, Loss: 3.2421720027923584\n",
      "Epoch 130, Loss: 3.0540828704833984\n",
      "Epoch 140, Loss: 2.889267921447754\n",
      "Epoch 150, Loss: 2.7428181171417236\n",
      "Epoch 160, Loss: 2.6044270992279053\n",
      "Epoch 170, Loss: 2.470330238342285\n",
      "Epoch 180, Loss: 2.3342368602752686\n",
      "Epoch 190, Loss: 2.1938750743865967\n",
      "Epoch 200, Loss: 2.056637763977051\n",
      "Epoch 210, Loss: 1.9218400716781616\n",
      "Epoch 220, Loss: 1.7908934354782104\n",
      "Epoch 230, Loss: 1.659375548362732\n",
      "Epoch 240, Loss: 1.5447407960891724\n",
      "Epoch 250, Loss: 1.4212068319320679\n",
      "Epoch 260, Loss: 1.3433170318603516\n",
      "Epoch 270, Loss: 1.2422605752944946\n",
      "Epoch 280, Loss: 1.1532038450241089\n",
      "Epoch 290, Loss: 1.0863066911697388\n",
      "Epoch 300, Loss: 1.0639783143997192\n",
      "Epoch 310, Loss: 1.0067254304885864\n",
      "Epoch 320, Loss: 0.9580923318862915\n",
      "Epoch 330, Loss: 0.9134228229522705\n",
      "Epoch 340, Loss: 0.8894293904304504\n",
      "Epoch 350, Loss: 0.8525689244270325\n",
      "Epoch 360, Loss: 0.822476863861084\n",
      "Epoch 370, Loss: 0.8395225405693054\n",
      "Epoch 380, Loss: 0.8564611673355103\n",
      "Epoch 390, Loss: 0.839884877204895\n",
      "Epoch 400, Loss: 0.7926021814346313\n",
      "Epoch 410, Loss: 0.7586004734039307\n",
      "Epoch 420, Loss: 0.7317605018615723\n",
      "Epoch 430, Loss: 0.7105302214622498\n",
      "Epoch 440, Loss: 0.6912007927894592\n",
      "Epoch 450, Loss: 0.6734093427658081\n",
      "Epoch 460, Loss: 0.6566593647003174\n",
      "Epoch 470, Loss: 0.6407175064086914\n",
      "Epoch 480, Loss: 0.6254581212997437\n",
      "Epoch 490, Loss: 0.6107836961746216\n",
      "Epoch 500, Loss: 0.596626877784729\n",
      "Epoch 510, Loss: 0.5859625339508057\n",
      "Epoch 520, Loss: 0.5722132325172424\n",
      "Epoch 530, Loss: 0.5568852424621582\n",
      "Epoch 540, Loss: 0.5454603433609009\n",
      "Epoch 550, Loss: 0.5352908968925476\n",
      "Epoch 560, Loss: 0.5255028009414673\n",
      "Epoch 570, Loss: 0.5158334970474243\n",
      "Epoch 580, Loss: 0.5073235630989075\n",
      "Epoch 590, Loss: 0.4978695213794708\n"
     ]
    }
   ],
   "source": [
    "for i in range (num_epochs): \n",
    "    logits = casual_language_model(input_tokens)\n",
    "    loss = F.cross_entropy(\n",
    "    logits.view(-1, logits.shape[-1]),  # shape: (batch_size * seq_len, vocab_size)\n",
    "    target_tokens.reshape(-1)  )         # shape: (batch_size * seq_len,)) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if i % 10 == 0: \n",
    "        print(f\"Epoch {i}, Loss: {loss.item()}\") \n",
    "        pred = logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c20f4c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = casual_language_model(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9412868e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3, 10,  9, 12, 18, 11,  2,  0,  0],\n",
       "        [ 3, 10,  9,  8,  7,  4,  2,  0,  0],\n",
       "        [ 3,  8,  7,  4, 18, 13,  2,  0,  0],\n",
       "        [ 3,  6, 12,  9,  8, 14, 12,  6,  2],\n",
       "        [ 3, 12,  6, 12, 16,  9,  8,  2,  0]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2ef0cd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start>\n",
      "New token: dont (36.28%)\n",
      "New sequence:  <start> dont\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming 'input_str', 'encode', 'decode', 'causal_language_model', and 'device' are defined elsewhere\n",
    "\n",
    "input_str = \"<start>\"\n",
    "# Encode the input string into tokens, convert to a PyTorch tensor,\n",
    "# move it to the specified device (e.g., CPU or GPU), and add a batch dimension.\n",
    "input_tokens = torch.tensor(encode(input_str)).to(device).unsqueeze(0)\n",
    "\n",
    "# Pass the input tokens through the causal language model to get logits\n",
    "# Logits are raw, unnormalized scores for each possible next token.\n",
    "logits = casual_language_model(input_tokens)\n",
    "\n",
    "# Get the probability distribution of the last token's prediction\n",
    "# logits[0, -1:] selects the logits for the last token in the sequence.\n",
    "# softmax(dim=-1) converts these logits into probabilities.\n",
    "last_token_pred = logits[0, -1:].softmax(dim=-1)\n",
    "\n",
    "# Get the index of the most probable next token (the predicted token)\n",
    "# argmax(dim=-1) finds the index of the maximum value along the last dimension.\n",
    "# keepdim=True maintains the dimension, so it remains a tensor of shape (1, 1).\n",
    "last_token_logits = last_token_pred.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "# Concatenate the new predicted token to the original input tokens to form a new sequence\n",
    "# dim=1 means concatenate along the sequence length dimension.\n",
    "new_sequence = torch.cat([input_tokens, last_token_logits], dim=1)\n",
    "\n",
    "# Decode the last predicted token index back into a human-readable string\n",
    "# .tolist()[0] converts the tensor to a Python list and gets the first (and only) element.\n",
    "last_predicted_token = decode(last_token_logits.tolist()[0])\n",
    "\n",
    "# Print the input string\n",
    "print(\"Input:\", input_str)\n",
    "# Print the new token and its confidence (maximum probability)\n",
    "print(f\"New token: {last_predicted_token} ({(last_token_pred.max().item() * 100):.2f}%)\")\n",
    "# Print the entire new sequence of tokens decoded back into a string\n",
    "print(\"New sequence: \", decode(new_sequence[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ebe77d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(prefix, max_length=18): \n",
    "    input_tokens = torch.tensor([vocab_to_index[\"<start>\"]] + encode(prefix)).to(device).unsqueeze(0) \n",
    "    for _ in range(max_length): \n",
    "        with torch.no_grad(): \n",
    "            logits = casual_language_model(input_tokens)\n",
    "            last_token_logits = logits[0, -1:].argmax(dim=-1, keepdim=True)\n",
    "            print(decode([last_token_logits.tolist()[0][0]]))\n",
    "            input_tokens = torch.cat((input_tokens, last_predicted_token))\n",
    "        if input_tokens[0][-1] == vocab_to_index[\"<end>\"]: \n",
    "            break \n",
    "    return decode(input_tokens[0].tolist()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bf5db5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"dont forget to like and subscibe\",\n",
    "    \"dont forget machine learning is fun\",\n",
    "    \"machine learning is fun and awesome\",\n",
    "    \"if you like machine learning i like you\",\n",
    "    \"i like you more than machine learning\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "959f99de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(prefix, max_length=18): \n",
    "    input_tokens = torch.tensor([vocab_to_index[\"<start>\"]] + encode(prefix)).to(device).unsqueeze(0) \n",
    "    for _ in range(max_length): \n",
    "        with torch.no_grad(): \n",
    "            logits = casual_language_model(input_tokens)\n",
    "            last_token_logits = logits[0, -1:].argmax(dim=-1, keepdim=True)  # shape (1, 1)\n",
    "            print(decode([last_token_logits.tolist()[0][0]]))\n",
    "            input_tokens = torch.cat((input_tokens, last_token_logits), dim=1)\n",
    "        if input_tokens[0][-1].item() == vocab_to_index[\"<end>\"]: \n",
    "            break \n",
    "    return decode(input_tokens[0].tolist()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e0266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fd955674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "than\n",
      "machine\n",
      "learning\n",
      "<end>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<start> i like you more than machine learning <end>'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation(\"i like you more\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
