{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea9e249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import math \n",
    "import torch.nn.functional as F \n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04f14061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_masked_attention(\n",
    "        values: torch.Tensor,\n",
    "        keys: torch.Tensor,\n",
    "        query: torch.Tensor,\n",
    "        mask: torch.Tensor = None \n",
    "): \n",
    "    attention_scores = torch.matmul(query, keys.transpose(-2,-1)) \n",
    "    attention_scores = attention_scores / math.sqrt(keys.shape[-1])\n",
    "    if mask is not None: \n",
    "        attention_scores = torch.where(mask == 0, torch.tensor(-1e9), attention_scores) \n",
    "    attention_scores = F.softmax(attention_scores, dim=-1) \n",
    "    attention = torch.matmul(attention_scores, values) \n",
    "    return attention, attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62574633",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module): \n",
    "    def __init__(self, embed_size: int):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(embed_size, embed_size)\n",
    "        self.layer2 = nn.Linear(embed_size, embed_size) \n",
    "    def forward(self, x): \n",
    "        x = self.layer1(x) \n",
    "        x = F.gelu(x)\n",
    "        x = self.layer2(x) \n",
    "        return x \n",
    "class AttentionLayer(nn.Module): \n",
    "    def __init__(self,embed_size: int):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size \n",
    "        self.query_dense = nn.Linear(embed_size, embed_size) \n",
    "        self.key_dense = nn.Linear(embed_size, embed_size) \n",
    "        self.value_dense = nn.Linear(embed_size, embed_size)\n",
    "        self.output_dense = nn.Linear(embed_size, embed_size) \n",
    "    def forward(self, embeddings: torch.Tensor): \n",
    "        batch_size = embeddings.shape[0] \n",
    "        seq_length = embeddings.shape[1] \n",
    "        query = self.query_dense[embeddings] \n",
    "        key = self.key_dense(embeddings)\n",
    "        value = self.value_dense(embeddings)\n",
    "        right_triangular_mask = torch.tril(torch.ones((1, seq_length, seq_length))).to(embeddings.device)\n",
    "        attention, attention_scores = calculate_masked_attention(value, key, query, right_triangular_mask) \n",
    "        return attention, attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c58a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module): \n",
    "    def __init__(self, embed_size: int):\n",
    "        super().__init__()\n",
    "        self.attention_layer = AttentionLayer(embed_size) \n",
    "        self.feed_forward = FeedForward(embed_size) \n",
    "        self.layer_norm1 = nn.LayerNorm(embed_size) \n",
    "    def forward(self, x: torch.Tensor): \n",
    "        context, attention_scores = self.attention_layer(x)\n",
    "        context = self.layer_norm1(context) \n",
    "        context = self.feed_forward(context) \n",
    "        context = F.gelu(context) \n",
    "        output = context + x \n",
    "        return output, attention_scores \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_size: int, num_layers: int): \n",
    "        super().__init__() \n",
    "        self.transformers_blocks = nn.ModuleList([TransformerBlock(embed_size) for _ in range(num_layers)]) \n",
    "    def forward(self, x: torch.Tensor): \n",
    "        attention_scores = [] \n",
    "        for transformer_block in self.transformers_blocks: \n",
    "            x, attention_score = transformer_block(x) \n",
    "            attention_scores.append(attention_score) \n",
    "        return attention_scores\n",
    "    \n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size: int, max_seq_length: int):\n",
    "        super().__init__() \n",
    "        position = torch.arange(max_seq_length).unsqueeze(1) \n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2) * (-math.log(1000.0) / embed_size))\n",
    "        pe = torch.zeros(20, embed_size) \n",
    "        pe[:, 0::2] = torch.sin(position * div_term) \n",
    "        pe[:, 1:: 2] = torch.cos(position * div_term) \n",
    "        self.register_buffer('position_embedding', pe)\n",
    "    def forward(self, x: torch.Tensor): \n",
    "        return x + self.position_embedding[:x.size(1), :] \n",
    "\n",
    "class CasualLanguageModel(nn.Module): \n",
    "    def __init__(self, embed_size: int, vocab_size: int, num_layers: int): \n",
    "        super().__init__() \n",
    "        self.embedding_layer = nn.Parameter(torch.randn(vocab_size, embed_size))\n",
    "        self.transformer = Transformer(embed_size, num_layers)\n",
    "        self.positional_encoding = SinusoidalPositionalEncoding(embed_size, max_seq_length=20) \n",
    "\n",
    "    def forward(self, x: torch.Tensor, return_attention_scores: bool = False): \n",
    "        x = torch.nn.functional.embedding(x, self.embedding_layer)\n",
    "        x = self.positional_encoding(x) \n",
    "        x, attention_scores = self.transformer(x) \n",
    "        logits = torch.matmul(x, self.embedding_layer.T) \n",
    "        if return_attention_scores: \n",
    "            return logits, attention_scores\n",
    "        return logits \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cce764cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<start>', '<end>', 'fun', 'forget', 'than', 'i', 'is', 'subscibe', 'awesome', 'more', 'like', 'and', 'dont', 'you', 'learning', 'to', 'machine', 'if']\n",
      "vocab size:  {'<pad>': 0, '<start>': 1, '<end>': 2, 'fun': 3, 'forget': 4, 'than': 5, 'i': 6, 'is': 7, 'subscibe': 8, 'awesome': 9, 'more': 10, 'like': 11, 'and': 12, 'dont': 13, 'you': 14, 'learning': 15, 'to': 16, 'machine': 17, 'if': 18}\n"
     ]
    }
   ],
   "source": [
    "dataset = [\n",
    "    \"dont forget to like and subscibe\",\n",
    "    \"dont forget machine learning is fun\",\n",
    "    \"machine learning is fun and awesome\",\n",
    "    \"if you like machine learning i like you\",\n",
    "    \"i like you more than machine learning\"\n",
    "]\n",
    "vocab = set() \n",
    "special_tokens = [\"<pad>\", \"<start>\", \"<end>\"] \n",
    "for sentence in dataset: \n",
    "    vocab.update(sentence.split())\n",
    "vocab = special_tokens + list(vocab) \n",
    "vocab_to_index = {word: index for index, word in enumerate(vocab)} \n",
    "vocab_size = len(vocab) \n",
    "print(vocab)\n",
    "print(\"vocab size: \", vocab_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e06d058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 13,  4, 16, 11, 12,  8,  2,  0,  0],\n",
       "        [ 1, 13,  4, 17, 15,  7,  3,  2,  0,  0],\n",
       "        [ 1, 17, 15,  7,  3, 12,  9,  2,  0,  0],\n",
       "        [ 1, 18, 14, 11, 17, 15,  6, 11, 14,  2],\n",
       "        [ 1,  6, 11, 14, 10,  5, 17, 15,  2,  0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(sentence: str): \n",
    "    return [vocab_to_index[word] for word in sentence.split()]\n",
    "def encode_batch(sentences: list[str]): \n",
    "    encoded_sentences = [[vocab_to_index[\"<start>\"]] + encode(sentence) + [vocab_to_index[\"<end>\"]] for sentence in sentences] \n",
    "    max_length = max([len(encoded_sentence) for encoded_sentence in encoded_sentences])\n",
    "    encoded_sentences = [encoded_sentence + [vocab_to_index[\"<pad>\"]] * (max_length - len(encoded_sentence)) for encoded_sentence in encoded_sentences ]\n",
    "    return encoded_sentences \n",
    "def decode(tokens: list[int]): \n",
    "    return \" \".join([vocab[token] for token in tokens]) \n",
    "tokenized_dataset = encode_batch(dataset) \n",
    "tokenized_dataset = torch.tensor(tokenized_dataset) \n",
    "(tokenized_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8646fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> dont forget to like and subscibe <end> <pad> <pad>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d01188e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 13,  4, 16, 11, 12,  8,  2,  0],\n",
      "        [ 1, 13,  4, 17, 15,  7,  3,  2,  0],\n",
      "        [ 1, 17, 15,  7,  3, 12,  9,  2,  0],\n",
      "        [ 1, 18, 14, 11, 17, 15,  6, 11, 14],\n",
      "        [ 1,  6, 11, 14, 10,  5, 17, 15,  2]])\n",
      "tensor([[13,  4, 16, 11, 12,  8,  2,  0,  0],\n",
      "        [13,  4, 17, 15,  7,  3,  2,  0,  0],\n",
      "        [17, 15,  7,  3, 12,  9,  2,  0,  0],\n",
      "        [18, 14, 11, 17, 15,  6, 11, 14,  2],\n",
      "        [ 6, 11, 14, 10,  5, 17, 15,  2,  0]])\n"
     ]
    }
   ],
   "source": [
    "input_tokens = tokenized_dataset[:, :-1] \n",
    "target_tokens = tokenized_dataset[:, 1:] \n",
    "print(input_tokens) \n",
    "print(target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd170d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  <start> dont forget to like and subscibe <end> <pad>\n",
      "Target:  dont forget to like and subscibe <end> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \", decode(input_tokens[0].tolist())) \n",
    "print(\"Target: \", decode(target_tokens[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b2850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab) \n",
    "embed_size = 6 \n",
    "num_layers = 2\n",
    "device = \"cpu\"\n",
    "num_epochs = 600 \n",
    "input_tokens = input_tokens.to(device) \n",
    "target_tokens = target_tokens.to(device) \n",
    "casual_language_model = CasualLanguageModel(embed_size=embed_size, vocab_size=vocab_size, num_layers=num_layers).to(device)\n",
    "optimizer = torch.optim.Adam(casual_language_model.parameters(), lr=2e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad2222",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = casual_language_model(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164594b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for v, x in zip(vocab, logits[0][2].softmax(-1)): \n",
    "    print(v, x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c47bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits.shape) \n",
    "logits.view(-1, logits.shape[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcf7887",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_tokens.shape)\n",
    "print(target_tokens.view(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce19b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tokens.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59770dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), target_tokens.view(-1))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b02fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (num_epochs): \n",
    "    logits = casual_language_model(input_tokens)\n",
    "    loss = F.cross_entropy(logits.view(-1, logits.shape[-1], target_tokens.view(-1)))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if i % 10 == 0: \n",
    "        print(f\"Epoch {i}, Loss: {loss.item()}\") \n",
    "        pred = logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f4c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = casual_language_model(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9412868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef0cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming 'input_str', 'encode', 'decode', 'causal_language_model', and 'device' are defined elsewhere\n",
    "\n",
    "input_str = \"<start>\"\n",
    "# Encode the input string into tokens, convert to a PyTorch tensor,\n",
    "# move it to the specified device (e.g., CPU or GPU), and add a batch dimension.\n",
    "input_tokens = torch.tensor(encode(input_str)).to(device).unsqueeze(0)\n",
    "\n",
    "# Pass the input tokens through the causal language model to get logits\n",
    "# Logits are raw, unnormalized scores for each possible next token.\n",
    "logits = casual_language_model(input_tokens)\n",
    "\n",
    "# Get the probability distribution of the last token's prediction\n",
    "# logits[0, -1:] selects the logits for the last token in the sequence.\n",
    "# softmax(dim=-1) converts these logits into probabilities.\n",
    "last_token_pred = logits[0, -1:].softmax(dim=-1)\n",
    "\n",
    "# Get the index of the most probable next token (the predicted token)\n",
    "# argmax(dim=-1) finds the index of the maximum value along the last dimension.\n",
    "# keepdim=True maintains the dimension, so it remains a tensor of shape (1, 1).\n",
    "last_token_logits = last_token_pred.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "# Concatenate the new predicted token to the original input tokens to form a new sequence\n",
    "# dim=1 means concatenate along the sequence length dimension.\n",
    "new_sequence = torch.cat([input_tokens, last_token_logits], dim=1)\n",
    "\n",
    "# Decode the last predicted token index back into a human-readable string\n",
    "# .tolist()[0] converts the tensor to a Python list and gets the first (and only) element.\n",
    "last_predicted_token = decode(last_token_logits.tolist()[0])\n",
    "\n",
    "# Print the input string\n",
    "print(\"Input:\", input_str)\n",
    "# Print the new token and its confidence (maximum probability)\n",
    "print(f\"New token: {last_predicted_token} ({(last_token_pred.max().item() * 100):.2f}%)\")\n",
    "# Print the entire new sequence of tokens decoded back into a string\n",
    "print(\"New sequence: \", decode(new_sequence[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe77d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(prefix, max_length=18): \n",
    "    input_tokens = torch.tensor([vocab_to_index[\"<start>\"]] + encode(prefix)).to(device).unsqueeze(0) \n",
    "    for _ in range(max_length): \n",
    "        with torch.no_grad(): \n",
    "            logits = casual_language_model(input_tokens)\n",
    "            last_token_logits = logits[0, -1:].argmax(dim=-1, keepdim=True)\n",
    "            print(decode([last_token_logits.tolist()[0][0]]))\n",
    "            input_tokens = torch.cat((input_tokens, last_predicted_token))\n",
    "        if input_tokens[0][-1] == vocab_to_index[\"<end>\"]: \n",
    "            break \n",
    "    return decode(input_tokens[0].tolist()) \n",
    "generation(\"i like\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
