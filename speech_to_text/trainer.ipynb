{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6ef0c6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Could not find module 'C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\torchaudio\\lib\\libtorchaudio.pyd' (or one of its dependencies). Try using the full path with constructor syntax.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[0;32m      8\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_dataset, get_tokenzier \n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtranscribe_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TranscribeModel\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn \n",
      "File \u001b[1;32mc:\\Users\\Dell\\Documents\\att\\speech_to_text\\dataset.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m \n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchaudio\u001b[39;00m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m \n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchaudio\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize extension and backend first\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa  # usort: skip\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_backend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa  # usort: skip\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     AudioMetaData,\n\u001b[0;32m      5\u001b[0m     get_audio_backend,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     set_audio_backend,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     compliance,\n\u001b[0;32m     15\u001b[0m     datasets,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     utils,\n\u001b[0;32m     24\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchaudio\\_extension\\__init__.py:38\u001b[0m\n\u001b[0;32m     36\u001b[0m _IS_ALIGN_AVAILABLE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _IS_TORCHAUDIO_EXT_AVAILABLE:\n\u001b[1;32m---> 38\u001b[0m     \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchaudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_torchaudio\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     _check_cuda_version()\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchaudio\\_extension\\utils.py:60\u001b[0m, in \u001b[0;36m_load_lib\u001b[1;34m(lib)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_ops.py:1392\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m   1387\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[0;32m   1389\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[0;32m   1390\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[0;32m   1391\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[1;32m-> 1392\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ctypes\\__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Could not find module 'C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\torchaudio\\lib\\libtorchaudio.pyd' (or one of its dependencies). Try using the full path with constructor syntax."
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"PYTHON_ENABLE_MPS_FALLBACK\"] = \"1\" \n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "import torch \n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from dataset import get_dataset, get_tokenzier \n",
    "from transcribe_model import TranscribeModel\n",
    "from torch import nn \n",
    "\n",
    "vq_initial_loss_weight = 10 \n",
    "vq_warmup_steps = 1000 \n",
    "vq_final_loss_weight = 0.5 \n",
    "num_epochs = 1000 \n",
    "starting_steps = 0\n",
    "num_examples = 100 \n",
    "model_id = \"test37\"\n",
    "num_batch_repeats = 1 \n",
    "\n",
    "starting_steps = 0\n",
    "BATCH_SIZE = 64 \n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "def run_loss_function(log_probs, target, blank_token): \n",
    "    # Add log_softmax to ensure proper probability ditribution \n",
    "\n",
    "    loss_function = nn.CTCLoss(blank=blank_token) \n",
    "    input_lengths = tuple(log_probs.shape[1] for _ in range(log_probs.shape[0])) \n",
    "    target_lengths = (target != blank_token).sum(dim=1)\n",
    "    target_lengths = tuple(t.item() for t in target_lengths)\n",
    "    input_seq_first= log_probs.permute(1,0,2)\n",
    "    loss = loss_function(input_seq_first, target, input_lengths, target_lengths)\n",
    "    return loss \n",
    "\n",
    "def decode_predictions(output, tokenizer, blank_token):\n",
    "    \"\"\"Decode model predictions to text\"\"\"\n",
    "    predictions = []\n",
    "    batch_size = output.shape[0]\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Get the most likely tokens\n",
    "        pred_tokens = torch.argmax(output[i], dim=-1)\n",
    "        \n",
    "        # Remove blanks and consecutive duplicates (CTC decoding)\n",
    "        decoded_tokens = []\n",
    "        prev_token = None\n",
    "        for token in pred_tokens:\n",
    "            token_id = token.item()\n",
    "            if token_id != blank_token and token_id != prev_token:\n",
    "                decoded_tokens.append(token_id)\n",
    "            prev_token = token_id\n",
    "        \n",
    "        # Convert to text\n",
    "        try:\n",
    "            text = tokenizer.decode(decoded_tokens)\n",
    "            predictions.append(text)\n",
    "        except:\n",
    "            predictions.append(\"\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def main():\n",
    "    log_dir = f\"runs/speech2text_training/{model_id}\"\n",
    "    if os.path.exists(log_dir): \n",
    "        import shutil \n",
    "        shutil.rmtree(log_dir)\n",
    "    writer = SummaryWriter(log_dir) \n",
    "\n",
    "    tokenizer = get_tokenzier()\n",
    "    blank_token = tokenizer.token_to_id(\"â–¡\")\n",
    "\n",
    "    device = torch.device(\n",
    "        \"cuda\"\n",
    "         if torch.cuda.is_available() \n",
    "         else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    )\n",
    "    print(f\"Using device : {device}\")\n",
    "\n",
    "    if os.path.exists(f\"models/{model_id}/model_latest.pth\"): \n",
    "        print(f\"Loading model from models/{model_id}/model_latest.pth\") \n",
    "        model = TranscribeModel.load(f\"models/{model_id}/model_latest.pth\").to(device)\n",
    "    else:\n",
    "        model = TranscribeModel(\n",
    "            num_codebooks=2,\n",
    "            codebook_size=32,\n",
    "            embedding_dim=16,\n",
    "            num_transformer_layers=2,\n",
    "            vocab_size=len(tokenizer.get_vocab())\n",
    "        ).to(device)\n",
    "    num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Number of trainable parameter: {num_trainable_params}\") \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    dataloader = get_dataset(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_examples=num_examples,\n",
    "        num_workers=1 \n",
    "    )\n",
    "\n",
    "    ctc_losses = []\n",
    "    vq_losses = []\n",
    "    num_batches = len(dataloader)\n",
    "    steps = starting_steps \n",
    "\n",
    "    for i in range(num_epochs): \n",
    "        for idx, batch in enumerate(dataloader): \n",
    "            for repeatBatch in range(num_batch_repeats): \n",
    "                audio = batch[\"audio\"]\n",
    "                target = batch[\"input_ids\"]\n",
    "                text = batch[\"text\"] \n",
    "\n",
    "                if target.shape[1] > audio.shape[1]: \n",
    "                    print(\n",
    "                        \"Padding audio, target is longer than audio. Audio Shape: \",\n",
    "                        audio.shape,\n",
    "                        \"Target Shape: \",\n",
    "                        target.shape\n",
    "                    )\n",
    "                    audio = torch.nn.functional.pad(\n",
    "                        audio, (0,0,0,target.shape[1] - audio.shape[1])\n",
    "                    )\n",
    "                    print(\"After padding: \", audio.shape) \n",
    "                audio = audio.to(device)\n",
    "                target = target.to(device) \n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output, vq_loss = model(audio) \n",
    "                ctc_loss = run_loss_function(output, target, blank_token) \n",
    "                \n",
    "                #Calculate vq_loss_weight using linear warmup schedule \n",
    "                vq_loss_weight =  max(\n",
    "                    vq_final_loss_weight,\n",
    "                    vq_initial_loss_weight \n",
    "                    - (vq_initial_loss_weight - vq_final_loss_weight) \n",
    "                    * (steps / vq_warmup_steps)\n",
    "                )\n",
    "                if vq_loss is None: \n",
    "                    loss = ctc_loss\n",
    "                else:\n",
    "                    loss = ctc_loss + vq_loss_weight * vq_loss\n",
    "                if torch.isinf(loss): \n",
    "                    print(\"Loss is inf, skipping step\", audio.shape, target.shape) \n",
    "                    continue \n",
    "                loss.backward() \n",
    "                #Increase gradient clipping threshold \n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), max_norm=10.0\n",
    "                ) #changed from 1.0\n",
    "                optimizer.step() \n",
    "\n",
    "                ctc_losses.append(ctc_loss.item()) \n",
    "                vq_losses.append(vq_loss.item() if vq_loss is not None else 0)  # Fixed bug here\n",
    "                steps += 1\n",
    "\n",
    "                #Log to tensorboard every step\n",
    "                if steps % 20 == 0: \n",
    "                    avg_ctc_loss = sum(ctc_losses) / len(ctc_losses) \n",
    "                    avg_vq_loss = sum(vq_losses) / len(vq_losses) \n",
    "                    avg_loss = avg_ctc_loss + vq_loss_weight * avg_vq_loss\n",
    "                    print(\n",
    "                        f\"Num Steps: {steps}, Batch: {idx + 1}/{num_batches}, ctc_loss: {avg_ctc_loss:.3f}, vq_loss: {avg_vq_loss:.3f}, total_loss: {avg_loss:.3f}\"\n",
    "                    )\n",
    "\n",
    "                    # Log to tensorboard\n",
    "                    writer.add_scalar('Loss/CTC', avg_ctc_loss, steps)\n",
    "                    writer.add_scalar('Loss/VQ', avg_vq_loss, steps)\n",
    "                    writer.add_scalar('Loss/Total', avg_loss, steps)\n",
    "                    writer.add_scalar('Loss/VQ_Weight', vq_loss_weight, steps)\n",
    "\n",
    "                    # Generate transcription examples\n",
    "                    if steps % 20 == 0:\n",
    "                        model.eval()\n",
    "                        with torch.no_grad():\n",
    "                            # Use current batch for examples\n",
    "                            sample_output, _ = model(audio[:4])  # Use first 4 samples\n",
    "                            predictions = decode_predictions(sample_output, tokenizer, blank_token)\n",
    "                            ground_truths = text[:4]  # First 4 ground truth texts\n",
    "                            \n",
    "                            print(\"Transcription Examples\")\n",
    "                            print(\"=\" * 80)\n",
    "                            print(f\"{'Example #':<10} {'Model Output':<35} {'Ground Truth'}\")\n",
    "                            print(\"=\" * 80)\n",
    "                            \n",
    "                            for j, (pred, truth) in enumerate(zip(predictions, ground_truths)):\n",
    "                                print(f\"{j:<10} {pred:<35} {truth}\")\n",
    "                            \n",
    "                            print(\"=\" * 80)\n",
    "                        model.train()\n",
    "\n",
    "                    ctc_losses = []\n",
    "                    vq_losses = []\n",
    "\n",
    "                # Save model periodically\n",
    "                if steps % 100 == 0:\n",
    "                    os.makedirs(f\"models/{model_id}\", exist_ok=True)\n",
    "                    model.save(f\"models/{model_id}/model_latest.pth\")\n",
    "                    print(f\"Model saved at step {steps}\")\n",
    "\n",
    "    # Final model save\n",
    "    os.makedirs(f\"models/{model_id}\", exist_ok=True)\n",
    "    model.save(f\"models/{model_id}/model_final.pth\")\n",
    "    print(\"Training completed. Final model saved.\")\n",
    "    \n",
    "    # Close tensorboard writer\n",
    "    writer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
