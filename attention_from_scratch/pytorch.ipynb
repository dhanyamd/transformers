{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9c9a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import math \n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dd1bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_attention(\n",
    "    query: torch.Tensor,\n",
    "    keys: torch.Tensor,\n",
    "    values: torch.Tensor\n",
    "): \n",
    "    #perform matmul \n",
    "    attention_scores = torch.matmul(query, keys.transpose(-2,-1)) \n",
    "    attention_scores = attention_scores / math.sqrt(keys.shape[-1]) \n",
    "    attention = torch.matmul(attention_scores, values) \n",
    "    return attention, attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28970e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =2 \n",
    "num_queries = 4 \n",
    "num_keys = 16 \n",
    "embed_size = 8 \n",
    "query = torch.randn(batch_size, num_queries, embed_size) \n",
    "keys = torch.randn(batch_size, num_keys, embed_size) \n",
    "values = torch.randn(batch_size, num_keys, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd7fb31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ac83cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention, attention_scores = calculate_attention(query, keys, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0eaaade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 8]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape, attention_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd0aa243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'will': 0, 'attention!': 1, 'attention': 2, 'train': 3, '.': 4, 'we': 5}\n"
     ]
    }
   ],
   "source": [
    "text = \"attention! we will train attention .\" \n",
    "text_tokens = text.split() \n",
    "vocab = set(text_tokens) \n",
    "vocab_to_idx = {token: idx for idx, token in enumerate(vocab)} \n",
    "print(vocab_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efacda60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 5, 0, 3, 2, 4]]) \n",
      "shape: torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "int_tokens = torch.tensor([vocab_to_idx[token] for token in text_tokens]) \n",
    "int_tokens = int_tokens.unsqueeze(0) \n",
    "print(int_tokens, \"\\nshape:\", int_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b20d1c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ea4dd2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1434,  0.1184, -1.2842, -1.6180, -0.6949, -0.4059, -0.3229,\n",
       "           0.8431],\n",
       "         [-0.7437,  0.7823,  0.7492,  1.4523,  0.0660,  0.5996,  0.5312,\n",
       "          -1.4024],\n",
       "         [-0.2346, -0.2163,  0.0753, -0.9114,  0.5445, -0.6132, -0.7213,\n",
       "          -0.2497],\n",
       "         [ 0.4371, -0.2856, -0.9438, -1.5427, -1.0193,  0.1261, -0.9378,\n",
       "          -0.0200],\n",
       "         [ 0.9038, -0.8440,  0.1119,  0.2089, -0.2603,  1.6706,  0.9360,\n",
       "          -0.8233],\n",
       "         [ 1.5770, -0.6095, -0.3253,  1.2963,  1.5325, -0.6263,  0.5931,\n",
       "          -0.8159]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embedding_layer(int_tokens)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a97d9a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 8 \n",
    "embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embedding_dim) \n",
    "query_dense_layer = nn.Linear(in_features=embedding_dim, out_features=8) \n",
    "key_dense_layer = nn.Linear(in_features=embedding_dim, out_features=8) \n",
    "value_dense_layer = nn.Linear(in_features=embedding_dim, out_features=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c69de2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embedding_layer(int_tokens)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57f7b75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 8]), torch.Size([1, 6, 8]), torch.Size([1, 6, 8]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embedding_layer(int_tokens) \n",
    "query = query_dense_layer(embeddings)\n",
    "key = key_dense_layer(embeddings)\n",
    "value = key_dense_layer(embeddings) \n",
    "\n",
    "query.shape, key.shape, value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81c0e549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 8]), torch.Size([1, 6, 6]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention, attention_scores = calculate_attention(query, key, value)\n",
    "attention.shape, attention_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdd3e16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 0.],\n",
       "         [1., 1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_triangular_mask = torch.tril(torch.ones_like(attention_scores)) \n",
    "right_triangular_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0973d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_masked_attention(\n",
    "        values: torch.Tensor,\n",
    "        keys: torch.Tensor,\n",
    "        query: torch.Tensor,\n",
    "        mask: torch.Tensor = None \n",
    "): \n",
    "    attention_scores = torch.matmul(query, keys.transpose(-2,-1)) \n",
    "    attention_scores = attention_scores / math.sqrt(keys.shape[-1])\n",
    "    if mask is not None: \n",
    "        attention_scores = torch.where(mask == 0, torch.tensor(-1e9), attention_scores) \n",
    "    attention_scores = F.softmax(attention_scores, dim=-1) \n",
    "    attention = torch.matmul(attention_scores, values) \n",
    "    return attention, attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "651b3603",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_Context, attention_scores = calculate_masked_attention(query, key, value, right_triangular_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f8be463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 8])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_Context.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30628e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define dense layers for query, key, and value transformations\n",
    "# 'embedding_dim' would be the input feature dimension, and '8' is the output feature dimension.\n",
    "query_dense_layer_2 = nn.Linear(in_features=embedding_dim, out_features=8)\n",
    "key_dense_layer_2 = nn.Linear(in_features=embedding_dim, out_features=8)\n",
    "value_dense_layer_2 = nn.Linear(in_features=embedding_dim, out_features=8)\n",
    "\n",
    "# Apply the dense layers to 'attention_context' to get query, key, and value representations\n",
    "# 'attention_context' would be the input tensor to these layers.\n",
    "query_2 = query_dense_layer_2(attention_Context)\n",
    "key_2 = key_dense_layer_2(attention_Context)\n",
    "value_2 = value_dense_layer_2(attention_Context)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
